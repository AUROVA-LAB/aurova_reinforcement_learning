# Reference: https://github.com/DLR-RM/rl-baselines3-zoo/blob/master/hyperparams/ppo.yml#L32
seed: 64

n_timesteps: !!float 6e8      # Total timesteps in which the agent will learn
policy: 'MlpPolicy'           # Type of Policy
n_steps: 16                   # The number of steps to run for each environment per update (i.e. rollout buffer size is n_steps * n_envs where n_envs is number of environment copies running in parallel)
batch_size: 4096              # Minibatch size
gae_lambda: 0.95              # Factor for trade-off of bias vs variance for Generalized Advantage Estimator
gamma: 0.99                   # Discount factor
n_epochs: 20                  # Number of epoch when optimizing the surrogate loss
ent_coef: 0.01                # Entropy coefficient
learning_rate: !!float 1e-4
clip_range: !!float 0.2       # Clipping parameter for the value function
use_sde: True                 # Mandatory if "squash_output" is activated
policy_kwargs: "dict(
                  activation_fn=nn.Tanh,
                  net_arch=[256, 128, 64],
                  squash_output=True,
                  share_features_extractor = True,
                )"
vf_coef: 1.0                  # Value function coefficient for the loss calculation
max_grad_norm: 1.0            # The maximum value for the gradient clipping
device: "cpu"              # Device
# normalize_input: False        # Wether to normalize observations and action